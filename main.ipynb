{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import rdkit \n",
    "import multiprocessing\n",
    "import copy\n",
    "import math \n",
    "import random\n",
    "import pickle \n",
    "import utils \n",
    "import model \n",
    "from utils import parallel_f, get_mol, replace_atom, tokenize, pad, MyDataset, get_dic, clones\n",
    "from model import TransformerVAE, device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_smi_list(path) :\n",
    "    with open(path, 'r') as file :\n",
    "        return [smi[:-1] for smi in file.readlines()]\n",
    "    \n",
    "\n",
    "def replace_atom(smi) :\n",
    "    return smi.replace('Cl', 'L').replace('Br', 'R') \n",
    "\n",
    "def get_mol(smi) :\n",
    "    return rdkit.Chem.MolFromSmiles(smi)\n",
    "\n",
    "def parallel_f(f, input_list) :\n",
    "    pool = multiprocessing.Pool()\n",
    "    return pool.map(f, input_list)\n",
    "\n",
    "def get_dic(smi_list) :\n",
    "    dic = {'<START>': 0, '<END>': 1, '<PAD>': 2}\n",
    "    for smi in smi_list :\n",
    "        for char in smi :\n",
    "            if char not in dic :\n",
    "                dic[char] = len(dic) \n",
    "    return dic \n",
    "\n",
    "def tokenize(smi) :\n",
    "    return [0] + [smi_dic[char] for char in smi] + [1]\n",
    "\n",
    "def pad(smi) :\n",
    "    return smi + [2] * (max_len - len(smi))\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "\n",
    "def get_mask(target) :\n",
    "    mask = (target != smi_dic['<PAD>']).unsqueeze(-2)\n",
    "    return mask & subsequent_mask(target.size(-1)).type_as(mask.data)\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset) :\n",
    "    def __init__(self, token_list) :\n",
    "        self.token_list = token_list\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.token_list)\n",
    "\n",
    "    def __getitem__(self, idx) :   \n",
    "        return torch.tensor(self.token_list[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 373814\n"
     ]
    }
   ],
   "source": [
    "# Load ChEMBL dataset\n",
    "\n",
    "with open('data/chembl24_canon_train.pickle', 'rb') as file :\n",
    "    smi_list = pickle.load(file) \n",
    "\n",
    "\n",
    "smi_list = [smi for smi in smi_list if len(smi) < 40] # Choose only smiles with length < 40\n",
    "\n",
    "print(f'Number of data: {len(smi_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smi_list = get_smi_list('data/ADAGRASIB_SMILES.txt')\n",
    "smi_dic = get_dic(smi_list)\n",
    "\n",
    "inv_dic = {v:k for k, v in smi_dic.items()}\n",
    "\n",
    "token_list = parallel_f(tokenize, smi_list)\n",
    "max_len = len(max(token_list, key=len))\n",
    "token_list = parallel_f(pad, token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mask(target, smi_dic) :\n",
    "    mask = (target != smi_dic['<PAD>']).unsqueeze(-2)\n",
    "    return mask & subsequent_mask(target.size(-1)).type_as(mask.data)\n",
    "\n",
    "def frange_cycle_cosine(start, stop, n_epoch, n_cycle=4, ratio=0.5):\n",
    "    L = np.ones(n_epoch)\n",
    "    period = n_epoch/n_cycle\n",
    "    step = (stop-start)/(period*ratio) # step is in [0,1]\n",
    "    \n",
    "    # transform into [0, pi] for plots: \n",
    "\n",
    "    for c in range(n_cycle):\n",
    "\n",
    "        v , i = start , 0\n",
    "        while v <= stop:\n",
    "            L[int(i+c*period)] = 0.5-.5*math.cos(v*math.pi)\n",
    "            v += step\n",
    "            i += 1\n",
    "    return L    \n",
    "\n",
    "\n",
    "def gen_beta(start, end, T1, T2, T3):\n",
    "    for i in range(T1):\n",
    "        yield start\n",
    "    log_s = np.log(start)\n",
    "    log_e = np.log(end)\n",
    "    T = T2 - T1\n",
    "    AT = T3 - T1\n",
    "    for i in range(T):\n",
    "        cur_beta = np.exp(log_s + (log_e - log_s) / AT * i)\n",
    "        yield cur_beta\n",
    "\n",
    "    T = T3 - T2\n",
    "    delta_beta = (end - cur_beta) / T\n",
    "    for i in range(T):\n",
    "        cur_beta += delta_beta\n",
    "        yield cur_beta\n",
    "\n",
    "    while True:\n",
    "        yield end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "dataset = MyDataset(token_list)\n",
    "train_set, val_set = random_split(dataset, [0.9, 0.1])\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_set, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import math \n",
    "import utils\n",
    "from utils import get_mask, clones\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "    \n",
    "class Attention(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head) :\n",
    "        super(Attention, self).__init__()\n",
    "        assert dim_model % num_head == 0, 'dim_model % num_head != 0'\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_head = dim_model // num_head\n",
    "\n",
    "        self.Q = nn.Linear(dim_model, dim_model)\n",
    "        self.K = nn.Linear(dim_model, dim_model)\n",
    "        self.V = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask = None) :\n",
    "        B = Q.size(0) \n",
    "\n",
    "        Q, K, V = self.Q(Q), self.K(K), self.V(V)\n",
    "\n",
    "        len_Q, len_K, len_V = Q.size(1), K.size(1), V.size(1)\n",
    "\n",
    "        Q = Q.reshape(B, self.num_head, len_Q, self.dim_head)\n",
    "        K = K.reshape(B, self.num_head, len_K, self.dim_head)\n",
    "        V = V.reshape(B, self.num_head, len_V, self.dim_head)\n",
    "        \n",
    "        K_T = K.transpose(2,3).contiguous()\n",
    "\n",
    "        attn_score = Q @ K_T\n",
    "\n",
    "        attn_score = attn_score / (self.dim_head ** 1/2)\n",
    "        if mask is not None :\n",
    "            attn_score = attn_score.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_distribution = torch.softmax(attn_score, dim = -1)\n",
    "\n",
    "        attn = attn_distribution @ V\n",
    "\n",
    "        attn = attn.reshape(B, len_Q, self.num_head * self.dim_head)\n",
    "        \n",
    "        attn = self.out(attn)\n",
    "\n",
    "        return attn, attn_distribution\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class EncoderOne(nn.Module) :\n",
    "    def __init__(self, dim_model, dim_latent, num_head, num_layer, dropout) :\n",
    "        super(EncoderOne, self).__init__()\n",
    "\n",
    "        self.layers = clones(EncoderOneLayer(dim_model, dim_latent, num_head, dropout), num_layer)\n",
    "\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_latent),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(dim_latent),\n",
    "        )\n",
    "        self.sigma = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_latent),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(dim_latent),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        for layer in self.layers : \n",
    "            x = layer(x) \n",
    "\n",
    "        return self.mu(x), self.sigma(x) \n",
    "\n",
    "\n",
    "class EncoderOneLayer(nn.Module) :\n",
    "    def __init__(self, dim_model, dim_latent, num_head, dropout) :\n",
    "        super(EncoderOneLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.self_attn = Attention(dim_model, num_head) \n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.feed_foward = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_model),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_model, dim_model)\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(dim_model) \n",
    "\n",
    "    def forward(self, x) :\n",
    "        x = self.norm1(x)\n",
    "        attn, self_attn = self.self_attn(x, x, x)\n",
    "        x = x + self.drop1(attn)\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.feed_foward(x)\n",
    "        x = x + self.drop2(x)   \n",
    "\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "class EncoderTwo(nn.Module) :\n",
    "    def __init__(self, dim_model, dim_latent, num_head, num_layer, dropout) :\n",
    "        super(EncoderTwo, self).__init__()\n",
    "\n",
    "        self.layers = clones(EncoderTwoLayer(dim_latent, dim_latent, num_head, dropout), num_layer)\n",
    "        self.expand = EncoderTwoLayer(dim_model, dim_latent, num_head, dropout)\n",
    "\n",
    "    def forward(self, z) :\n",
    "        for layer in self.layers : \n",
    "            z = layer(z) \n",
    "        x = self.expand(z)\n",
    "        return x \n",
    "\n",
    "class EncoderTwoLayer(nn.Module) :\n",
    "    def __init__(self, dim_model, dim_latent, num_head, dropout) :\n",
    "        super(EncoderTwoLayer, self).__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_latent)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.self_attn = Attention(dim_latent, num_head) \n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim_latent)\n",
    "        self.feed_foward = nn.Sequential(\n",
    "            nn.Linear(dim_latent, dim_model),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(dim_model, dim_model)\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(dim_model)       \n",
    "\n",
    "    def forward(self, z) :\n",
    "        z = self.norm1(z)\n",
    "        attn, self_attn = self.self_attn(z, z, z)\n",
    "        z = z + self.drop1(attn)\n",
    "\n",
    "        z = self.norm2(z)\n",
    "        z = self.feed_foward(z)\n",
    "        z = z + self.drop2(z)   \n",
    "\n",
    "        z = self.norm3(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, dim_model, dim_expansion, num_head, num_layer, dropout, smi_dic) :\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = clones(DecoderLayer(dim_model, dim_expansion, num_head, dropout, smi_dic), num_layer) \n",
    "\n",
    "    def forward(self, memory, target, mask) :\n",
    "        for layer in self.layers : \n",
    "            target = layer(memory, target, mask) \n",
    "        return target\n",
    "\n",
    "class DecoderLayer(nn.Module) :\n",
    "    def __init__(self, dim_model, dim_expansion, num_head, dropout, smi_dic) :\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_model)   \n",
    "        self.self_attn = Attention(dim_model, num_head)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.cross_attn = Attention(dim_model, num_head)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(dim_model)\n",
    "        self.feed_foward = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_expansion),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(dim_expansion, dim_model),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, memory, target, mask) :\n",
    "        target = self.norm1(target)\n",
    "        attn, self_attn = self.self_attn(target, target, target, mask)\n",
    "        target = target + self.drop1(attn)\n",
    "\n",
    "        target = self.norm2(target)\n",
    "        attn, cross_attn = self.cross_attn(target, memory, memory)\n",
    "        target = target + self.drop2(attn)\n",
    "\n",
    "        target = self.norm3(target)\n",
    "        target = self.feed_foward(target)\n",
    "        target = target + self.drop3(target)\n",
    "\n",
    "        return target\n",
    "\n",
    "\n",
    "class TransformerVAE(nn.Module) :\n",
    "    def __init__(self, dim_model, dim_expansion, dim_latent, num_head, num_layer, dropout, smi_dic) :\n",
    "        super(TransformerVAE, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.smi_dic = smi_dic\n",
    "        \n",
    "        self.embed = nn.Embedding(len(smi_dic), dim_model)\n",
    "        self.pos = PositionalEncoding(dim_model, dropout)\n",
    "\n",
    "        self.encoder_1 = EncoderOne(dim_model, dim_latent, num_head, num_layer, dropout)\n",
    "\n",
    "        self.pos_z = PositionalEncoding(dim_latent, dropout)\n",
    "\n",
    "        self.encoder_2 = EncoderTwo(dim_model, dim_latent, num_head, num_layer, dropout)\n",
    "\n",
    "        self.embed_tgt = nn.Embedding(len(smi_dic), dim_model) \n",
    "        self.pos_tgt = PositionalEncoding(dim_model, dropout)\n",
    "\n",
    "        self.decoder = Decoder(dim_model, dim_expansion, num_head, num_layer, dropout, smi_dic) \n",
    "\n",
    "        self.norm = nn.LayerNorm(dim_model)\n",
    "        self.proj = nn.Linear(dim_model, len(smi_dic))\n",
    "\n",
    "    def reparameterization(self, mu, sigma) :\n",
    "        eps = torch.rand_like(sigma).to(device)\n",
    "        z = mu + torch.exp(sigma) * eps\n",
    "        return z \n",
    "\n",
    "    def get_mask(self, target, smi_dic) :\n",
    "        mask = (target != smi_dic['<PAD>']).unsqueeze(-2)\n",
    "        return mask & subsequent_mask(target.size(-1)).type_as(mask.data)\n",
    "    \n",
    "    def inference(self, z, target) :\n",
    "        z = self.pos_z(z) \n",
    "\n",
    "        memory = self.encoder_2(z) \n",
    "\n",
    "        mask = self.get_mask(target, self.smi_dic)\n",
    "        mask = mask.unsqueeze(1).to(device) \n",
    "\n",
    "        target = self.embed_tgt(target) * (self.dim_model ** 0.5)\n",
    "        target = self.pos_tgt(target)\n",
    "\n",
    "        target = self.decoder(memory, target, mask) \n",
    "\n",
    "        target = self.norm(target) \n",
    "        target = self.proj(target)\n",
    "        target = F.log_softmax(target, dim = -1)\n",
    "\n",
    "        return target\n",
    "    \n",
    "    def forward(self, x, target) :\n",
    "        x = self.embed(x) * (self.dim_model ** 0.5)   \n",
    "        x = self.pos(x) \n",
    "\n",
    "        mu, sigma = self.encoder_1(x) \n",
    "        z = self.reparameterization(mu, sigma) \n",
    "\n",
    "        z = self.pos_z(z) \n",
    "\n",
    "        memory = self.encoder_2(z) \n",
    "\n",
    "        mask = self.get_mask(target, self.smi_dic)\n",
    "        mask = mask.unsqueeze(1).to(device) \n",
    "\n",
    "        target = self.embed_tgt(target) * (self.dim_model ** 0.5)\n",
    "        target = self.pos_tgt(target)\n",
    "\n",
    "        target = self.decoder(memory, target, mask) \n",
    "\n",
    "        target = self.norm(target) \n",
    "        target = self.proj(target)\n",
    "        target = F.log_softmax(target, dim = -1)\n",
    "\n",
    "        return target, mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerVAE(dim_model=256,\n",
    "                       dim_expansion=256,\n",
    "                       dim_latent = 128,\n",
    "                       num_head=8,\n",
    "                       num_layer=2,\n",
    "                       dropout=0.5,\n",
    "                       smi_dic=smi_dic).to(device)\n",
    "\n",
    "# loss_fn = nn.NLLLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.0003) \n",
    "\n",
    "\n",
    "def loss_fn(pred, tgt, mu, sigma, beta) :\n",
    "    reconstruction_loss = F.nll_loss(pred.reshape(-1, len(smi_dic)), tgt[:, 1:].reshape(-1), reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp()) \n",
    "    return  reconstruction_loss + kl_loss * beta \n",
    "NUM_EPOCH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O)1Occcccccccccccccccccccccccccccccccccc\n",
      "cccccccccccccccccccccccccccccccccccccccc\n",
      "cccccccccccccccccccccccccccccccccccccccc\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "C(=O)ccccccccccccccccccccccccccccccccccc\n",
      "C(=O)ccccccccccccccccccccccccccccccccccc\n",
      "C(=O)ccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "C(=O)CCC(=O)CC(=O)CC(=O)C(=O)C(=O)C)CCC(\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "C(=O)ccccccccccccccccccccccccccccccccccc\n",
      "C(=O)ccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "C(=O)ccccccccccccccccccccccccccccccccccc\n",
      "Cccccccccccccccccccccccccccccccccccccccc\n",
      "C(=O)ccccccccccccccccccccccccccccccccccc\n",
      "C(=O)ccccccccccccccccccccccccccccccccccc\n",
      "C(=O)ccccccccccccccccccccccccccccccccccc\n",
      "CC1C(=O)C1cccccccccccccccccccccccccccccc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) :\n\u001b[0;32m---> 27\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     _, idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(out, \u001b[38;5;241m1\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "Cell \u001b[0;32mIn [8], line 268\u001b[0m, in \u001b[0;36mTransformerVAE.inference\u001b[0;34m(self, z, target)\u001b[0m\n\u001b[1;32m    264\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_z(z) \n\u001b[1;32m    266\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_2(z) \n\u001b[0;32m--> 268\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmi_dic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m    271\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tgt(target) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_model \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[0;32mIn [8], line 261\u001b[0m, in \u001b[0;36mTransformerVAE.get_mask\u001b[0;34m(self, target, smi_dic)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, target, smi_dic) :\n\u001b[1;32m    260\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (target \u001b[38;5;241m!=\u001b[39m smi_dic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask \u001b[38;5;241m&\u001b[39m \u001b[43msubsequent_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype_as(mask\u001b[38;5;241m.\u001b[39mdata)\n",
      "Cell \u001b[0;32mIn [3], line 35\u001b[0m, in \u001b[0;36msubsequent_mask\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubsequent_mask\u001b[39m(size):\n\u001b[1;32m     34\u001b[0m     attn_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, size, size)\n\u001b[0;32m---> 35\u001b[0m     subsequent_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype(\n\u001b[1;32m     36\u001b[0m         torch\u001b[38;5;241m.\u001b[39muint8\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m subsequent_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "beta_np_cyc = frange_cycle_cosine(0.0, 1.0, NUM_EPOCH, 1)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCH + 1) :\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    beta = beta_np_cyc[epoch-1]\n",
    "    for i, input in enumerate(train_loader) :\n",
    "        model.train()\n",
    "\n",
    "        input = input.to(device)\n",
    "        pred, mu, sigma = model(input, input[:, :-1])\n",
    "\n",
    "        loss = loss_fn(pred, input, mu, sigma, beta)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        if random.random() < 0.5: \n",
    "            z = torch.randn(1, max_len, 128).to(device)\n",
    "            target = torch.zeros(1, 1, dtype=torch.long).to(device)\n",
    "\n",
    "            for i in range(max_len - 1) :\n",
    "                out = model.inference(z, target)\n",
    "                _, idx = torch.topk(out, 1, dim = -1)\n",
    "                idx = idx[:, -1, :]\n",
    "                target = torch.cat([target, idx], dim = 1)\n",
    "\n",
    "            target = target.squeeze(0).tolist()\n",
    "            smiles = ''.join([inv_dic[i] for i in target])\n",
    "            smiles = smiles.replace(\"<START>\", \"\").replace(\"<PAD>\", \"\").replace(\"<END>\",\"\")\n",
    "            # valid = \"Valid\" if get_mol(smiles) else \"Not\"\n",
    "            print(f'{smiles}')\n",
    "    print(f'epoch : {epoch}, train loss : {train_loss / len(train_loader)} val loss : {val_loss / len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:36:28] SMILES Parse Error: syntax error while parsing: CA\n",
      "[16:36:28] SMILES Parse Error: Failed parsing SMILES 'CA' for input: 'CA'\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# To ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Alternatively, you can specify the type of warning to ignore\n",
    "# For example, to ignore DeprecationWarnings:\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Now, import RDKit without being bothered by warnings\n",
    "get_mol(\"CA\")\n",
    "# Your RDKit code here\n",
    "\n",
    "# After your RDKit code, you might want to reset the warning filter\n",
    "warnings.resetwarnings()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
